{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CS 522 - Advanced Data Mining - Spring 2016 - Final Project\n",
    "\n",
    "#                Gender Classification using Twitter Feeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step - 3) Tokenize the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import all required packages\n",
    "import pickle\n",
    "import re\n",
    "from itertools import product\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sampleClass14 import sampleClass14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampleClass = sampleClass14()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load these tweets\n",
    "tweets = pickle.load(open('tweets.pkl', 'rb'))\n",
    "junk_tweets = pickle.load(open('junk_tweets.pkl', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test tweet:\n",
      "\tdescr=find your greatness ‚Ä¢ ROC CHEER&SOFTBALL ‚Ä¢ #GarmoStrong\n",
      "\ttext=you SUCK üç≠\n",
      "user name: \n",
      " karlyn kelley\n"
     ]
    }
   ],
   "source": [
    "test_tweet = tweets[10]\n",
    "print 'test tweet:\\n\\tdescr=%s\\n\\ttext=%s' % (test_tweet['user']['description'],test_tweet['text'])\n",
    "print 'user name: \\n %s' %(test_tweet['user']['name']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test junk tweet:\n",
      "\tdescr=Father, Husband, Coach, Trainer, Member of Omega Psi Phi, Mentor, Pro Natural Bodybuilder,Former NFL, CFL, and Arena League Football Player.\n",
      "\ttext=Keep putting in work #swac #hbcu https://t.co/pdGaGY74ac\n",
      "junk user name: \n",
      " Coach Hicks\n"
     ]
    }
   ],
   "source": [
    "test_junk_tweet = junk_tweets[10]\n",
    "print 'test junk tweet:\\n\\tdescr=%s\\n\\ttext=%s' % (test_junk_tweet['user']['description'], test_junk_tweet['text'])\n",
    "print 'junk user name: \\n %s' %(test_junk_tweet['user']['name']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn']\n"
     ]
    }
   ],
   "source": [
    "print stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Father, Husband, Coach, Trainer, Member of Omega Psi Phi, Mentor, Pro Natural Bodybuilder,Former NFL, CFL, and Arena League Football Player.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "try:\n",
    "    # Wide UCS-4 build\n",
    "    myre = re.compile(u'['\n",
    "        u'\\U0001F300-\\U0001F64F'\n",
    "        u'\\U0001F680-\\U0001F6FF'\n",
    "        u'\\U0001f602'\n",
    "        u'\\u2600-\\u26FF\\u2700-\\u27BF\\ufe0f]+', \n",
    "        re.UNICODE)\n",
    "except re.error:\n",
    "    # Narrow UCS-2 build\n",
    "    myre = re.compile(u'('\n",
    "        u'\\ud83c[\\udf00-\\udfff]|'\n",
    "        u'\\ud83d[\\udc00-\\ude4f\\ude80-\\udeff]|'\n",
    "        u'[\\u2600-\\u26FF\\u2700-\\u27BF])+', \n",
    "        re.UNICODE)\n",
    "    \n",
    "print myre.sub('',test_junk_tweet['user']['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(string, lowercase, keep_punctuation, prefix,collapse_urls, collapse_mentions,collapse_stop_words):\n",
    "    if not string:\n",
    "        return []\n",
    "    if lowercase:\n",
    "        string = string.lower()\n",
    "    tokens = []\n",
    "    if collapse_urls:\n",
    "        string = re.sub('http\\S+', 'THIS_IS_A_URL', string)\n",
    "    if collapse_mentions:\n",
    "        string = re.sub('@\\S+', 'THIS_IS_A_MENTION', string)\n",
    "    if keep_punctuation:\n",
    "        tokens = string.split()\n",
    "    else:\n",
    "        tokens = re.sub('\\W+', ' ', string).split()\n",
    "    if collapse_stop_words:\n",
    "        tokens = list(set(tokens) - set(stopwords.words('english')))\n",
    "    if prefix:\n",
    "        tokens = ['%s%s' % (prefix, t) for t in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#five separate people almost hit my car in a matter of 30 seconds. \n",
    "#the students at Lamar need to take a driving course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep putting in work #swac #hbcu https://t.co/pdGaGY74ac\n",
      "[u'Keep', u'putting', u'in', u'work', u'swac', u'hbcu', u'https', u't', u'co', u'pdGaGY74ac']\n",
      "[u'work', u'keep', u'#hbcu', u'putting', u'#swac', u'THIS_IS_A_URL']\n"
     ]
    }
   ],
   "source": [
    "print junk_tweets[10]['text']\n",
    "print sampleClass.tokenize(junk_tweets[10]['text'], False, False, None,False, False,False)\n",
    "print sampleClass.tokenize(junk_tweets[10]['text'], True, True, None,True, True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'SUCK']\n"
     ]
    }
   ],
   "source": [
    "print sampleClass.tokenize(tweets[10]['text'], False, False, None,False, False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you SUCK üç≠\n",
      "[u'you', u'SUCK']\n",
      "[u'you', u'SUCK']\n"
     ]
    }
   ],
   "source": [
    "print tweets[10]['text']\n",
    "print sampleClass.tokenize(tweets[10]['text'], False, False, None,False, False,False)\n",
    "print sampleClass.tokenize(tweets[10]['text'], False, False, None,True, True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet2tokens(tweet, use_descr=True, lowercase=True,keep_punctuation=True, descr_prefix='d=',\n",
    "                 collapse_urls=True, collapse_mentions=True, use_text=True,collapse_stop_words = True):\n",
    "    tokens = []\n",
    "    if use_text:\n",
    "        tokens.extend(tokenize(tweet['text'], lowercase, keep_punctuation, None,collapse_urls, collapse_mentions,collapse_stop_words))\n",
    "    if use_descr:\n",
    "        tokens.extend(tokenize(tweet['user']['description'], lowercase,keep_punctuation, descr_prefix,collapse_urls, collapse_mentions,collapse_stop_words))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'work',\n",
       " u'keep',\n",
       " u'#hbcu',\n",
       " u'putting',\n",
       " u'#swac',\n",
       " u'THIS_IS_A_URL',\n",
       " u'arena',\n",
       " u'league',\n",
       " u'psi',\n",
       " u'natural',\n",
       " u'pro',\n",
       " u'phi,',\n",
       " u'player.',\n",
       " u'football',\n",
       " u'husband,',\n",
       " u'nfl,',\n",
       " u'trainer,',\n",
       " u'mentor,',\n",
       " u'member',\n",
       " u'father,',\n",
       " u'bodybuilder,former',\n",
       " u'coach,',\n",
       " u'omega',\n",
       " u'cfl,']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet2tokens(test_junk_tweet,True,True,True,None,True,True,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'work',\n",
       " u'keep',\n",
       " u'#hbcu',\n",
       " u'putting',\n",
       " u'#swac',\n",
       " u'THIS_IS_A_URL',\n",
       " u'arena',\n",
       " u'league',\n",
       " u'psi',\n",
       " u'natural',\n",
       " u'pro',\n",
       " u'phi,',\n",
       " u'player.',\n",
       " u'football',\n",
       " u'husband,',\n",
       " u'nfl,',\n",
       " u'trainer,',\n",
       " u'mentor,',\n",
       " u'member',\n",
       " u'father,',\n",
       " u'bodybuilder,former',\n",
       " u'coach,',\n",
       " u'omega',\n",
       " u'cfl,']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleClass.tweet2tokens(test_junk_tweet,True,True,True,None,True,True,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_descr_opts = [True, False]\n",
    "lowercase_opts = [True, False]\n",
    "keep_punctuation_opts = [True, False]\n",
    "descr_prefix_opts = ['d=', '']\n",
    "url_opts = [True, False]\n",
    "mention_opts = [True, False]\n",
    "use_text_opts = [True, False]\n",
    "stop_words_opts = [True, False]\n",
    "\n",
    "argnames = ['use_descr', 'lower', 'punct', 'prefix', 'url', 'mention', 'use_text', 'collapse_stop_words']\n",
    "option_iter = product(use_descr_opts, lowercase_opts,keep_punctuation_opts,descr_prefix_opts, url_opts,\n",
    "                       mention_opts, use_text_opts,stop_words_opts)\n",
    "\n",
    "# Write the output of various options to a file\n",
    "f = open('options.txt', 'w')\n",
    "for options in option_iter:\n",
    "    f.write('\\t'.join('%s=%s' % (name, opt) for name, opt in zip(argnames, options)))\n",
    "    f.write('\\n---------------------------\\n')\n",
    "    f.write(myre.sub('','    '.join(tweet2tokens(test_tweet, *options))))\n",
    "    f.write('\\n========================================================================\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write the output of various options of junk tweets to a file\n",
    "use_descr_opts = [True, False]\n",
    "lowercase_opts = [True, False]\n",
    "keep_punctuation_opts = [True, False]\n",
    "descr_prefix_opts = ['d=', '']\n",
    "url_opts = [True, False]\n",
    "mention_opts = [True, False]\n",
    "use_text_opts = [True, False]\n",
    "stop_words_opts = [True, False]\n",
    "\n",
    "argnames = ['use_descr', 'lower', 'punct', 'prefix', 'url', 'mention', 'use_text', 'collapse_stop_words']\n",
    "option_iter = product(use_descr_opts, lowercase_opts,keep_punctuation_opts,descr_prefix_opts, url_opts,\n",
    "                       mention_opts, use_text_opts,stop_words_opts)\n",
    "\n",
    "\n",
    "f1 = open('options_junk_tweets.txt', 'w')\n",
    "for options in option_iter:\n",
    "    f1.write('\\t'.join('%s=%s' % (name, opt) for name, opt in zip(argnames, options)))\n",
    "    f1.write('\\n---------------------------\\n')\n",
    "    f1.write(myre.sub('','    '.join(tweet2tokens(test_junk_tweet, *options))))\n",
    "    f1.write('\\n========================================================================\\n')\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets we have so far =  10000\n"
     ]
    }
   ],
   "source": [
    "print 'Total tweets we have so far = ',len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens_list = [sampleClass.tweet2tokens(t, use_descr=True, lowercase=True,keep_punctuation=False, descr_prefix='d=',\n",
    "                            collapse_urls=True, collapse_mentions=True,collapse_stop_words=True)\n",
    "              for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens_junk_list = [sampleClass.tweet2tokens(t, use_descr=True, lowercase=True,keep_punctuation=False, descr_prefix='d=',\n",
    "                            collapse_urls=True, collapse_mentions=True,collapse_stop_words=True)\n",
    "              for t in junk_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'swac', u'work', u'keep', u'putting', u'hbcu', u'THIS_IS_A_URL', u'd=trainer', u'd=league', u'd=phi', u'd=coach', u'd=natural', u'd=pro', u'd=nfl', u'd=father', u'd=cfl', u'd=arena', u'd=football', u'd=member', u'd=player', u'd=bodybuilder', u'd=psi', u'd=mentor', u'd=omega', u'd=former', u'd=husband']\n"
     ]
    }
   ],
   "source": [
    "print tokens_junk_list[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Vocabulary (A dictionary from term to index)\n",
    "\n",
    "def make_vocabulary(tokens_list):\n",
    "    vocabulary = defaultdict(lambda: len(vocabulary)) # Similar to vocabulary = defaultdict(int)\n",
    "    for tokens in tokens_list:\n",
    "        for token in tokens:\n",
    "            vocabulary[token]\n",
    "    print '%d unique terms in vocabulary' % len(vocabulary)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28340 unique terms in vocabulary\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sampleClass.make_vocabulary(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28340 unique terms in vocabulary\n"
     ]
    }
   ],
   "source": [
    "# How big is vocabulary if there is no punctuation\n",
    "vocabulary = make_vocabulary(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1855 unique terms in vocabulary\n"
     ]
    }
   ],
   "source": [
    "vocabulary_junk_tweets = sampleClass.make_vocabulary(tokens_junk_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "43019 unique terms in vocabulary\n",
      "Top 30 values in vocabulary are : [(u'd=senojl', 43018), (u'd=keona', 43017), (u'd=1968.', 43016), (u'd=orchard', 43015), (u'trek.', 43014), (u'#max.', 43013), (u'spin', 43012), (u'wheels!', 43011), (u'letters', 43010), (u'd=mixx', 43009), (u'eder', 43008), (u'perez', 43007), (u'employees', 43006), (u'd=gfhs', 43005), (u'd=#proera47', 43004), (u'b4.da.$$', 43003), (u'caffiene', 43002), (u'jitters', 43001), (u'trains!\"', 43000), (u'd=sandymadanatprTHIS_IS_A_MENTION', 42999), (u'destin', 42998), (u'grinding.', 42997), (u'd=astralis', 42996), (u'd=convincing', 42995), (u'd=1976.', 42994), (u'peachtree', 42993), (u'#moveslikejagger', 42992), (u'd=teenage', 42991), (u'd=mutant', 42990), (u'd=turtle!!!', 42989)]\n"
     ]
    }
   ],
   "source": [
    "# How big is vocabulary if we keep punctuation?\n",
    "tokens_list = [tweet2tokens(t, use_descr=True, lowercase=True,keep_punctuation=True, descr_prefix='d=',\n",
    "                            collapse_urls=True, collapse_mentions=True,collapse_stop_words=True)\n",
    "              for t in tweets]\n",
    "print len(tokens_list)\n",
    "\n",
    "vocabulary = make_vocabulary(tokens_list)\n",
    "d = Counter(vocabulary)\n",
    "print \"Top 30 values in vocabulary are :\" ,d.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "47062 unique terms in vocabulary\n",
      "Top 30 values in vocabulary are : [(u'politian', 47061), (u'cross-pollination?', 47060), (u'critic', 47059), (u'with...', 47058), (u'd=4.3.17', 47057), (u'#atl', 47056), (u\"lol'd\", 47055), (u'oversight', 47054), (u'demonstrative', 47053), (u'abdelrazik,', 47052), (u'case,', 47051), (u'fitting', 47050), (u'd=bestfriend\\u2764\\ufe0f', 47049), (u'place\\U0001f629', 47048), (u'nervous', 47047), (u'd=savvy?', 47046), (u'world!', 47045), (u'tweeting,', 47044), (u'least!', 47043), (u'doctrine', 47042), (u'issue.', 47041), (u'talking!!', 47040), (u\"d=vibin'\", 47039), (u'automatic', 47038), (u'd=gmen,', 47037), (u'd=wiz,', 47036), (u'd=nats,', 47035), (u'd=terps,', 47034), (u'd=#onedematha', 47033), (u'unsupervised', 47032)]\n"
     ]
    }
   ],
   "source": [
    "# How big is vocabulary if we keep punctuation and urls?\n",
    "tokens_list = [sampleClass.tweet2tokens(t, use_descr=True, lowercase=True,keep_punctuation=True, descr_prefix='d=',\n",
    "                            collapse_urls=False, collapse_mentions=True,collapse_stop_words=True)\n",
    "              for t in tweets]\n",
    "print len(tokens_list)\n",
    "\n",
    "vocabulary = sampleClass.make_vocabulary(tokens_list)\n",
    "d = Counter(vocabulary)\n",
    "print \"Top 30 values in vocabulary are :\" ,d.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "53557 unique terms in vocabulary\n",
      "Top 30 values in vocabulary are : [(u'politian', 53556), (u'cross-pollination?', 53555), (u'with...', 53554), (u'critic', 53553), (u'@mikesfilmtalk', 53552), (u'@copromote', 53551), (u'd=4.3.17', 53550), (u'#atl', 53549), (u'@lilrobzzz', 53548), (u\"lol'd\", 53547), (u'oversight', 53546), (u'@kuhn_eyesopen\\u2019s', 53545), (u'demonstrative', 53544), (u'abdelrazik,', 53543), (u'case,', 53542), (u\".@ararmaher's\", 53541), (u'fitting', 53540), (u'd=bestfriend\\u2764\\ufe0f', 53539), (u'place\\U0001f629', 53538), (u'nervous', 53537), (u'd=savvy?', 53536), (u'world!', 53535), (u'tweeting,', 53534), (u'least!', 53533), (u'@adriennesmith40', 53532), (u'doctrine', 53531), (u'issue.', 53530), (u'talking!!', 53529), (u\"d=vibin'\", 53528), (u'@cbeauregard8', 53527)]\n"
     ]
    }
   ],
   "source": [
    "# How big is vocabulary if we keep punctuation and urls and mentions?\n",
    "tokens_list = [sampleClass.tweet2tokens(t, use_descr=True, lowercase=True,keep_punctuation=True, descr_prefix='d=',\n",
    "                            collapse_urls=False, collapse_mentions=False,collapse_stop_words=True)\n",
    "              for t in tweets]\n",
    "print len(tokens_list)\n",
    "\n",
    "vocabulary = sampleClass.make_vocabulary(tokens_list)\n",
    "d = Counter(vocabulary)\n",
    "print \"Top 30 values in vocabulary are :\" ,d.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53816 unique terms in vocabulary\n",
      "Top 30 values in vocabulary are : [(u'politian', 53815), (u'with...', 53814), (u'critic', 53813), (u'cross-pollination?', 53812), (u'@copromote', 53811), (u'@mikesfilmtalk', 53810), (u'd=4.3.17', 53809), (u'#atl', 53808), (u\"lol'd\", 53807), (u'@lilrobzzz', 53806), (u'oversight', 53805), (u'demonstrative', 53804), (u'@kuhn_eyesopen\\u2019s', 53803), (u'abdelrazik,', 53802), (u'case,', 53801), (u\".@ararmaher's\", 53800), (u'fitting', 53799), (u'd=bestfriend\\u2764\\ufe0f', 53798), (u'place\\U0001f629', 53797), (u'nervous', 53796), (u'd=savvy?', 53795), (u'least!', 53794), (u'tweeting,', 53793), (u'world!', 53792), (u'@adriennesmith40', 53791), (u'issue.', 53790), (u'doctrine', 53789), (u'talking!!', 53788), (u\"d=vibin'\", 53787), (u'@cbeauregard8', 53786)]\n"
     ]
    }
   ],
   "source": [
    "# How big is vocabulary if we keep punctuation and urls and mentions and stop words?\n",
    "tokens_list = [sampleClass.tweet2tokens(t, use_descr=True, lowercase=True,keep_punctuation=True, descr_prefix='d=',\n",
    "                            collapse_urls=False, collapse_mentions=False,collapse_stop_words=False)\n",
    "              for t in tweets]\n",
    "\n",
    "\n",
    "vocabulary = sampleClass.make_vocabulary(tokens_list)\n",
    "d = Counter(vocabulary)\n",
    "print \"Top 30 values in vocabulary are :\" ,d.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 53816)\n"
     ]
    }
   ],
   "source": [
    "X1 = sampleClass.make_feature_matrix(tokens_list,vocabulary,len(tweets))\n",
    "print X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a CSR feature matrix to map each tweet to the frequency of it token appearance\n",
    "# X[i,j] is the frequency of term j in tweet i\n",
    "\n",
    "def make_feature_matrix(tokens_list, vocabulary):\n",
    "    X = lil_matrix((len(tweets), len(vocabulary)))\n",
    "    for i, tokens in enumerate(tokens_list):\n",
    "        for token in tokens:\n",
    "            j = vocabulary[token]\n",
    "            X[i,j] += 1\n",
    "    return X.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: (10000, 53816)\n"
     ]
    }
   ],
   "source": [
    "X = make_feature_matrix(tokens_list, vocabulary)\n",
    "print 'shape of X:', X.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
